{
	"documents": {
		"AkMSUZNA3Q21ldV0EONIDR": {
			"category": ["fundamentals", "statistics"],
			"content": "Expectation-Maximization algorithm is used to estimate the underlying probability distribution of (observed) incomplete data. The term “incomplete data” in its general form implies the existence of two sample spaces. The observed data is a realization from a latent space.Intuitive Explanation of EMWe want to maximize the posterior probability of the parameters $\\Theta$ given the data $X$, marginalizing over $\\mathbf{z}$:  $X$: observed dataset  $\\Theta$: the set of parameters, $\\Theta = { \\Theta_1, \\dots, \\Theta_K }$ where $\\Theta_k ={ \\theta_1, \\dots, \\theta_n }$  $n$: # of parameters of the distribution (If Gaussian, $n=2$, $\\mu$ and $\\sigma$.)  $K$: # of latent variables  $\\mathbf{z}$: latent variables, $\\mathbf{z} = { 1, \\dots, K }$  Summary      Input: observed data $X$, # of latent variables    Output: parameter set $\\Theta = {\\Theta_1, \\dots, \\Theta_K }$  CalculationThe following figure represents the calculation of EM algorithm with Gaussian Mixture Model(GMM), assuming that, for example, there are three latent variables(mixture components).References  A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models - Bilmes (1998)  The Expectation Maximization Algorithm - Frank Dellaert (2002)  What is the expectation maximization algorithm - Chuong B Do &amp; Serafim Batzoglou - npg (2008)  Scaling EM (Expectation-Maximization) Clustering to Large Databases - Bradley and Fayyad (1998)See Also  http://en.wikipedia.org/wiki/Expectation_maximization  http://sens.tistory.com/304",
			"date": 1375196400000,
			"tags": ["fundamentals", "statistics", "em"],
			"title": "Expectation Maximization, EM"
		},
		"BiskrkaJ7fZ8OvS9TqUL9O": {
			"category": ["algorithm"],
			"content": "In computer science, MinHash (or the min-wise independent permutations locality sensitive hashing scheme) is a technique for quickly estimating how similar two sets are. The scheme was invented by Andrei Broder (1997), and initially used in the AltaVista search engine to detect duplicate web pages and eliminate them from search results. It has also been applied in large-scale clustering problems, such as clustering documents by the similarity of their sets of words.",
			"date": 1378479600000,
			"tags": ["algorithm", "minhash"],
			"title": "MinHash"
		},
		"BsPitj3s9YBmqahKl5dJrM": {
			"category": ["algorithm"],
			"content": "Frequent PatternA pattern that occurs frequently in a data set.Association RulesFind all the rules $X \\rightarrow Y$ with minimum support (MinSup) and confidence.  $\\text{Support}(s)$: probability that a transaction contains both (or more) of two itemsets ($X \\cup Y$)  $\\text{Confidence}(c)$: conditional probability that a transaction having an itemset $X$ also contains another itemset $Y$AprioriApriori is a classic algorithm for frequent itemset mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.If there is any infrequent itemset, its superset should not be generated/tested.  Initially, scan DB once to get frequent $1$-itemset  Generate length ($k+1$) candidate itemsets from length $k$ frequent itemsets - Self Joining  Test the candidates against DB - Pruning  Terminate when no frequent or candidate set can be generatedReferences  “Data Mining: Concepts and Techniques” - Jiawei Han and Micheline Kamber",
			"date": 1378566000000,
			"tags": ["apriori"],
			"title": "Frequent Pattern Analysis"
		},
		"CIHIkGgu1pfpM5FaPttj6u": {
			"category": ["fundamentals", "statistics"],
			"content": "Binomial Distribution이항 분포는 Bernoulli 시행을 $n$번 독립적으로 반복했을 때의 random variable $X$가 따르는 분포이다. 다시 말하면, true(1)/false(0)의 두 가지 상태로 표현할 수 있는 사건(binary variable)에 대해 여러 시행을 거쳐 특정 observation(관측 결과)이 발생할 확률을 구할 때 사용된다.Conjugate Prior주어진 데이터(observation $X$)가 형성하는 분포의 파라미터($\\theta$)를 추정할 때 posterior probability $p(\\theta|X)$를 구한다. 베이즈 정리(Bayes’ rule)에서는 likelihood $p(X|\\theta)$와 prior $p(\\theta)$로 posterior를 구해내는데, 미리 posterior가 어떤 분포를 따르는지 알고 있다면 문제 해결은 상대적으로 간편해진다.여기서, 특정 분포를 따르는 likelihood에 대해서 posterior가 prior와 동일한 분포를 따르면 posterior probability를 쉽게 계산할 수 있다. 이러한 prior를 likelihood의 conjugate prior라고 한다. 즉, 문제를 해결하는 과정에서 likelihood의 conjugate prior를 취함으로써 posterior의 분포도 알 수 있고(prior의 분포와 같으므로), 계산도 쉬워지는 것이다.            Likelihood $p(X|\\theta)$      Conjugate prior $p_0(\\theta)$      Posterior $p(\\theta|X)$                  $Normal(\\mu, \\sigma)$      $Normal(\\mu_0, \\sigma_0)$      $Normal(\\mu_1, \\sigma_1)$              $Binomial(n,p)$      $Beta(\\alpha, \\beta)$      $Beta(\\alpha+n, \\beta+N-n)$              $Poisson(\\lambda)$      $Gamma(\\alpha, \\beta)$      $Gamma(\\alpha+n, \\beta+1)$              $Multinomial(p_1,\\dots,p_k)$      $Dirichlet(\\alpha_1,\\dots,\\alpha_k)$      $Dirichlet(\\alpha_1+n_1, \\dots, \\alpha_k+n_k)$      Beta Distribution베타 분포는 이산 분포(discrete)인 이항 분포와는 다르게 연속 분포(continuous)이다. (참고로 이 속성은 데이터 공학에서 smoothing에 이용되곤 한다.)Likelihood가 binomial distribution을 따를 때 likelihood의 conjugate prior가 바로 beta distribution이다. 즉, beta distribution에 binomial distribution을 결합하면 다시 beta distribution이 나온다는 것이다.",
			"date": 1375282800000,
			"tags": ["statistics", "bayesian"],
			"title": "Conjugate Prior"
		},
		"DzYgDZMoI0XdLggBuIT6P4": {
			"category": ["algorithm"],
			"content": "MotivationBefore introducing the basic concept of PageRank, we should first consider why we need web search engines. Due to the huge scale of the web, it is really difficult to find web pages we want. So, we use web search engines, and they optimize our search to find suitable web pages. The web search engines work as follows:  Basic principle of common Web search engines      Elicit search results by matching a query and keywords of web pages    Sort the results with relative importance of web pages  However, there are a few weak points of prior web search engines. Many of the web search engines, for example, Yahoo!, at that time, simply had utilized incoming-link count as a measure of higher quality search results or more important web pages. Since this way method disregards the importance of web pages, this sort of simple citation counting does not correspond to our common sense notion of importance. Moreover, this way can be easily abused through manipulation. Because it is so easy to make an arbitrary web page on the internet. That’s why the PageRank method had been proposed.Intuition of PageRankPageRank computes relative importance for every web page and assigns a rank to each web page. Considering the importance of a web page, we can say a web page is important when many other people also say the web page is important. And, the rank is a numeric value which represents the importance of a web page. (The goal of PageRank is to calculate out the ranks of every web page which exist on the Internet.)PageRank agrees that a highly linked web page tends to be more “important” than web pages with few incoming-links. In addition to this, PageRank says that a web page is truly important when it is linked by important web pages frequently.Web GraphIn order to find ranks of web pages, PageRank utilizes the link structure of the web, where web pages are linked each other by hyperlinks. If we build a graph model to represent the link structure of the web, we can make it easier to understand and solve a problem by using properties of a graph model.In the web graph model, a web page corresponds to a vertex, and a hyperlink corresponds to an edge:With respect to the hyperlink $e$, we can say that web page $u$ references web page $v$. In terms of web page $u$, the hyperlink $e$ is a forward link. On the other hand, in terms of web page $v$, the hyperlink $e$ is a backlink.Simplified PageRankNow, let’s figure out how the PageRank algorithm works. In PageRank, the rank propagates from a web page to other pages through hyperlinks. In fact, a hyperlink pointing to a web page can be interpreted as having trust in the contents of the web page. So, how does the rank propagate through hyperlinks?The rank propagates in the way as follows:  $R(u)$: the rank of web page $u$  $B_u$: the set of web pages which reference web page $u$  $F_v$: the set of web pages which the web page $v$ references  $N_v$: the number of forward links of web page $v$The rank of a web page is determined by the sum of all received ranks from its backlinks. And the rank is distributed evenly by the number of its forward links.Such rank propagation performs iteratively. This is to consider not only the influences from web pages located nearby but also the influences of web pages located far away.So far, we verified how the rank of a web page is calculated. Now, we can represent the rank calculations overall web graph by using the matrix notation.  $R_i$: a rank vector at $i$-th iteration  $S$: the transition kernel where the sum of entries in a row is $1$For your reference, the reason why the transposed stochastic matrix is used, is to adjust the direction of process that the rank of a web page, which is determined by the sum of given ranks from backlinks.Problems with Simplified PageRankDangling Links ProblemDangling nodes cannot deliver their ranks to other web pages. A dangling node is a web page with no forward link, and a dangling link is a hyperlink pointing to dangling nodes. Therefore, dangling nodes keep losing their ranks and thus the sum of ranks in the overall system keep decreasing.However, dangling links problem can be easily solved by adding virtual links from dangling nodes to all web pages.The virtual links enable dangling nodes to deliver their ranks to other web pages. The simplified PageRank formula is modified as follows:Vector $\\textbf{w}$ represents virtual links, and vector $\\textbf{d}$ represents existence of dangling nodes. So, the transition kernel is modified to this looking:RankSink ProblemAnother problem is the RankSink Problem. This occurs when some web page references to one of web pages that form a loop.This loop will accumulate ranks during iteration but never distribute any rank. So, the web pages in a loop become extremely important. On the other hand, the sum of rank outside the loop relatively keep decreasing.The RankSink problem is also solved by adding virtual links to all web pages.This way, the web pages in a loop can deliver their ranks to outside the loop through the virtual links. So, the modified PageRank formula is modified once more like this:The scalar $\\alpha$ is the probability of moving to random web pages, and the vector $\\textbf{w}$ represents virtual links.Random Surfer ModelSo far, we have drawn the complete formula of PageRank. In fact, the definition of PageRank has another intuitive basis in random walks on graphs. PageRank is based on intuitive behaviors of a real web surfer. A real web surfer can simply keep clicking on successive links at random and also jump to other web pages through bookmarks or by typing URL. These are named Random walk and Random jump respectively. (This is also called Random Walk with Restart, RWR.)Termination of ComputationPageRank computation terminates when it converges. Convergence means that the value of $||R_{i+1}-R_i||_1$ gets close to zero. So, we may wonder if the PageRank calculation always converges. The prerequisite for convergence of iterative calculation is that the stochastic matrix of PageRank should have all positive entries. (This is called “regular”.) It is defined in Markov chain:Markov ChainThe Markov chain is the general model of a system that changes from state to state.  $\\textbf{x}^{(n)}$: $n$-th state vector  $P$: stochastic transition matrixThe calculation converges only when the matrix $P$ has all positive entries.Implementation of PageRankNote that the $d$ factor increases the rate of convergence and maintains $||R||_1$. An alternative normalization is to multiply $R$ by the appropriate factor. The use of $d$ may have a small impact on the influence of $E$.Because $A$ is a huge size of sparse matrix, the $L1$-norm of $R_{i+1}$ becomes smaller than the $L1$-norm of the prior vector, $R_i$, after the matrix calculation. So, $d &gt; 0$.Personalized PageRankThe personalized PageRank is one of applications. It can provide personalized search results. By modifying the PageRank formula a little bit to random jump toward one target, web pages which are closely connected with the target will receive relatively high rank. We can consider this as a concept of recommendation.References  Page et al. - 1998 - The PageRank Citation Ranking Bringing Order to the Web  Yan, Lee - 2007 - Toward Alternative Measures for Ranking Venues A Case of Database Research CommunitySee Also  http://en.wikipedia.org/wiki/PageRank",
			"date": 1377615600000,
			"tags": ["pagerank"],
			"title": "PageRank"
		},
		"FExJKxqIfocDL9ww2E8xnp": {
			"category": ["data_modeling"],
			"content": "Topic model문서들의 집합에서 topic들을 찾아내기 위한 모델로, 눈에 보이는 observation, 즉, given data에 대해 통계적인 방법을 사용하여 모델을 생성하고, 새로운 데이터에 대해서 해당 모델을 적용시켜 원하는 문제를 해결한다.위에 그림에서 power라는 단어는 정치 토픽으로 쓰여 “국력”을 나타내는 것을 확인할 수 있다. 하지만 power라는 단어만 떼어 놓고 봤을 때, 사람과 관련해서 체력, 지구력 등을 나타낼 때 쓰일 수 있고, 과학에서는 중력, 구심력을 나타내는 표현에 쓰일 수 있고, 수학에서는 지수승을 의미하는 단어로 쓰일 수 있을 것이다. 이렇게 토픽 모델은 문서 내에 특정 단어가 어떤 의미로 쓰였는지 구분해주는 모델이다.토픽 모델을 사용하면 문서의 내용을 간결하게 나타낼 수 있고, 단어 및 문서 간의 유사도도 평가 가능하다. 그리고 문서 데이터에만 국한되는 것이 아니라 여러 분야에 쓰일 수 있는데, 정보 검색(IR), 인공 지능(AI), 바이오 인포메틱스 등에 다양하게 응용될 수 있다.Probabilistic Latent Semantic AnalysisPLSA에서는 observation에 영향을 끼치는 latent variable (topic)의 존재를 가정한다. 그리고 아래 그림과 같이 문서-단어 쌍 $(d,w)$을 observation으로 보고 observation의 생성 확률 $p(d,w)$을 구하고자 한다.PLSA 모델에서, asymmetric model로 설명하자면, 문서가 주어지고, 문서마다 다양한 topic을 다룰 수 있으며, 그 토픽에 따라 사용될 단어들이 결정된다. (하나의 문서 내에서 각 단어들은 하나의 topic하고만 연관되어 있음) 다시 말하면, 각 문서들은 여러 topic들의 mixture로 나타나고 $p(z \\vert d)$는 topic들의 mixing weight로서의 역할을 한다. 이 확률값들은 어디까지나 observation, 즉, $n(d,w)$에 의해 영향을 받게 될 것이다.Model Fitting with EM AlgorithmEM 알고리즘을 통한 model fitting 과정을 알아보자. 우리의 최종 목표는 주어진 데이터 (observation)에 맞는 모델을 생성하는 것, 즉, observation을 제일 잘 나타내는 확률 분포를 찾는 것인데, 주어진 데이터는 multinomial distribution을 따른다고 가정한다:Multinomial distribution에서의 parameter $\\Theta$는 ${p(d_1,w_1) , \\dots, p(d_N,w_M)}$이다. 그런데 PLSA모델에서는 latent variable인 topic $z$의 존재를 가정하고 observation과 topic 간의 확률적 연결을 정의하였기 때문에 다음과 같이 두 가지 버전으로 다시 쓸 수 있다:그리고나서 Expectation-Maximization 과정을 반복 수행하여 이 parameter의 수렴값을 구해내는 것이다. Expectation-Maxization 과정은 다음과 같다:  Expectation    Maximization  Weak PointsPLSA에서 문서 $d$는 $p(z \\vert d)$를 mixing weight로 하는 topic $z$의 mixture로 나타나는데, 문서셋 전체에 걸쳐 나타나는 topic distribution $p(z \\vert d)$의 경향까지는 나타내지 못한다. 다시 말하면, 모든 문서들은 uniformly 생성된다는 것이다. (There is no generative probabilistic model for the mixing proportions for topics.) 이게 문제가 되는 이유는, 전반적인 토픽의 분포 경향을 분포함수 등으로 나타내지 못하기 때문에 각각의 topic distribution에 대한 정보를 discrete하게 모두 가지고 있어야 한다는 것이다. 그러므로 PLSA의 모델 parameter 수는 주어지는 데이터(training set)의 크기에 linear하게 증가하게 되고($kN+kM$개), 이는 모델이 주어진 데이터에 지나치게 맞춰지는 overfitting의 문제에 이르게 된다. Overfitting이 왜 문제가 되느냐하면 생성된 model이 training data에 너무 맞춰져서 새로운 데이터에 모델을 적용할 수 없다는 것이다. 아래 curve fitting의 예로 overfitting이 왜 안 좋은 지 살펴보자.우리가 원하는 이상적인 모델은 green line이다. 하지만 주어진 데이터에 지나치게 맞춰지는 overfitting이 발생하면 모델이 마지막 네 번째 그림에서의 red line처럼 나타날 수 있다. ($M$은 red line의 차수로서, 토픽 모델의 파라미터의 개수에 대응되는 개념이다.) 이렇게 되면 새로운 데이터에 그 모델을 적용할 수 없게 되는 문제가 발생하는 것이다. PLSA 모델은 overfitting에 매우 취약하다. 그래서 본 논문에서는 tempering method(simulated annealing의 역전략)를 제안하지만, overfitting은 여전히 발생할 수 있다.[3] 그래서 discrete한 multinomial parameter space에 (continuous) prior distribution을 부여하는 방법이 제안되는데 그게 바로 LDA 모델이다.Summing Up      Input: $N×M$ document/word co-occurrence matrix $(d,w)$, # of topics $K$    Output: parameters $\\Theta = { p(d \\vert z); p(w \\vert z); p(z) }$  References  Hofmann - 1999 - Probabilistic latent semantic analysis  Hofmann - 1999 - Probabilistic latent semantic indexing  Popescul et al. - 2001 - Probabilistic Models for Unified Collaborative and Content-Based Recommendation in Sparse-Data EnvironmentsSee Also  http://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis",
			"date": 1375628400000,
			"tags": ["statistics", "data_modeling", "topic_model", "plsa"],
			"title": "Probabilistic Latent Semantic Analysis, PLSA"
		},
		"Fp1SUF5wBGVXsFW6nbHp36": {
			"category": ["fundamentals"],
			"content": "Singular Value Decomposition, SVDIn linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. Formally, the singular value decomposition of an $M \\times N$ real or complex matrix $A$ is a factorization of the formwhere $U$ is a $M \\times M$ real or complex unitary matrix, $\\Sigma$ is a $M \\times N$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and $V^T$ (the conjugate transpose of $V$) is a $N \\times N$ real or complex unitary matrix. The diagonal entries $\\Sigma_{i,j}$ are known as the singular values of $A$.Intuition  Rotation with $U$  Scaling with $\\Sigma$  Rotation with $V^T$CalculationSequence of computing: $A^TA \\rightarrow \\Sigma \\rightarrow V^T \\rightarrow U$  Compute $A^T$ and $A^TA$.  Determine the eigenvalues of the matrix $A^TA$ and sort these in descending order, in the absolute sense. Square roots these to obtain the singular values of $A$.  Construct diagonal matrix $\\Sigma$ by placing singular values in descending order along its diagonal. Compute its inverse, $\\Sigma^{-1}$.  Use the ordered eigenvalues from Step #2 and compute the eigenvectors of $A^TA$. Place these eigenvectors along the columns of $V$ and compute its transpose, $V^T$.  Compute $U$ as $U = AV\\Sigma^{-1}$. To compute the proof, compute the full $\\Sigma VD$ using $A = U\\Sigma V^T$.Low-Rank ApproximationIn mathematics, low-rank approximation is a minimization problem which is used for mathematical modeling and data compression.A best $k$-rank approximation, $\\hat{A}_{r=k}$ is given by zeroing out the $r-k$ trailing singular values of $A$, that isExampleThe SVD is given by $A = U\\Sigma V^T$, withThis matrix is rank $r = 3$. The rank-two approximation is given by zeroing out the smallest singular value, which producesReferences  https://inst.eecs.berkeley.edu/~ee127a/book/login/l_svd_main.htmlSee Also  http://en.wikipedia.org/wiki/Singular_value_decomposition  http://en.wikipedia.org/wiki/Low_rank_approximation",
			"date": 1378479600000,
			"tags": ["fundamentals", "linear_algebra", "svd"],
			"title": "Singular Value Decomposition, SVD"
		},
		"HEtxLPW4skvR59FgldDSco": {
			"category": ["fundamentals", "statistics"],
			"content": "Introduction베이지안 접근방식에서 가장 어려운 점은 추론 과정에서 다차원의 함수에 대해 integration을 요한다는 것이다. 여기서 소개할 MCMC(Markov chain Monte Carlo) method는 우리의 관심의 대상이 되는 (복잡한) target distribution(예를 들어, 앞에서 말한 posterior distribution)으로부터 표본을 채취(sampling)하여 그 (target) distribution을 simulation하는 방법이다.Monte Carlo IntegrationMCMC에 대해 살펴보기 전에 먼저 Monte Carlo method (줄여서 MC method)에 대해 알아보자. 본래 MC method은 난수를 생성하여 적분을 계산하기 위한 방법이었다.예를 들어, 아래 그림과 같이 $\\pi$의 값을 맞추기 위해 난수를 발생시키고 파란색의 점과 빨간색의 점의 갯수를 세서 $\\pi$의 값을 알아내는 방식이 Monte Carlo method이다. 그리고, 생성된 난수의 개수가 많을수록 더욱 정확하게 $\\pi=3.141592\\cdots$를 근사하는 것을 알 수 있다.이제 좀 더 자세히 베이지안 통계학에서 Monte Carlo method가 어떻게 수행되는지 살펴보자. 우리의 관심의 대상이 되는 값 $\\theta$가 다음과 같을 때,$h(x)$를 임의의 함수 $f(x)$와 $(a,b)$구간에서 정의된 어느 특정 확률분포 $p(x)$의 곱으로 표현할 수 있다면 다음과 같이 쓸 수 있다:결국, integral은 $x$에 관한 특정 함수 $f(x)$에 대한 기대값(expectation)으로 나타낼 수 있다. 이것을 Monte Carlo integration이라고 부른다. 뒤에서 다루겠지만, 이 Monte Carlo method는 베이지안 통계학에서 posterior distribution을 근사하여 parameter를 estimation하는 데에 사용될 수 있다.  Importance Sampling  Suppose that a distribution $q(x)$ is used to approximate the target distribution $p(x)$. Then we can describe as:    This form is called the importance sampling and we can say that    where $x_i$ is a sample drawn from the distribution $q(x)$ which approximates $p(x)$.Markov chain Monte Carlo, MCMC임의의 분포 $p(x)$가 있다고 가정하자. (여기서 분포 $p$는 univariate 또는 multivariate이다.) 이 분포를 추정하기 위해 해당 분포로부터 직접 sampling을 수행하는 것이 앞에서 말한 Monte Carlo method이다. 하지만 해당 분포로부터 직접 random number generation이 매우 어려운 경우에는 어떻게 할까?MCMC는 이러한 Monte Carlo method의 단점을 보완한 방법으로 해당 분포로부터 직접 random number generation 하는 대신에, stationary distribution을 가지는 Markov chain $p(x^{(i)}\\vert x^{(i-1)})$을 만들어 random number generation을 수행한다. MCMC에 대해 설명하기 이전에 Markov chain에 대해 먼저 알아보자.  Markov chain  Markov chain is a sequence of random variables $x^{(0)},x^{(1)},x^{(2)},\\cdots$ with the Markov property.  Markov property  The transition probability is dependent on only the current state, not on any previous states.  만약 우리의 관심의 대상이 되는 $x$에 대해서 현재 상태의 $x^{(i)}$값이 이전 상태의 $x^{(i-1)}$값에 dependent 하고, 이에 대한 확률 $p(x^{(i)}\\vert x^{(i-1)})$이 결국 수렴된다면 우리는 MCMC를 사용할 수 있다.MCMC가 수행되는 과정은 다음과 같다:      Step #1: Markov chain ${ x^{(0)}, x^{(1)}, \\cdots, x^{(M)} }$을 만든다. ($x^{(M)}$값은 stationary distribution을 따른다.)    Step #2: Step #1을 $n$번 반복한다. ($n$개의 $x^{(M)}$들이 density function $p(x)$를 따르는 random sample들이다.)  좋은 random number generation을 위해서는 step #1에서 $M$를 충분히 크게 해 주어야 하고, 이 과정은 초기값 $x^{(0)}$에 영향을 받지 않게 하는 과정으로 burn-in이라고 부른다.Gibbs samplingGibbs sampling은 두 개 이상의 변수들의 joint distribution으로부터 연쇄적으로 표본을 추출하는데, 앞에서 말한 MCMC에서 target distribution을 추정할 때 매우 중요한 역할을 한다.Gibbs sampling을 사용하려면 joint distribution으로부터 각 변수들에 대한 full conditional distribution을 모두 구해내야 한다.  Full conditional distribution  Gibbs sampling은 다음과 같이 수행된다:      Initialization: Set $X^{(0)} = { x_1^{(0)}, x_2^{(0)}, \\cdots, x_n^{(0)} }$ as some initial values.    Step #1: Sample $X^{(i)}$ from the full conditional distributions.          Step #2: Back to Step #1    Gibbs sampling for $n$-dimensional distribution $p(X)$이렇게 추출된 표본들은 모든 변수들의 joint distribution을 근사한다. 그리고, 특정 변수에 대한 기대값은 모든 표본들을 averaging함으로써 근사할 수 있다. 만약 burn-in 후 $M$개의 sample들이 생성되었다면, $M$이 커지면 커질수록 다음을 만족한다.ExampleSuppose that $X_1,X_2, \\cdots, X_n \\sim N(\\mu,\\sigma^2)$, where both parameters are unknown. The prior information is given bywhere $k$ is a constant. Provide a Gibbs algorithm to compute $E[g(\\mu,\\sigma^2)\\vert X]$, where $g$ is a function.The joint distribution is as follows:Now, we can derive full conditional distributions for each of parameters from the joint distribution described above.Then, we set $\\mu^{(0)}$,${\\sigma^2}^{(0)}$ as initial values and draw samples from the two full conditional distributions.After the sufficient number($M$) of iterations, we can get samples for each of parameters:In practice, it is necessarily needed to perform the burn-in and the auto-correlation function (ACF) test steps. In this example, we assume that we have performed the two steps and the burn-in point is at $5000$th iteration and there is no auto-correlation and $M = 10000$. Based on this assumption, we can calculate the values:Finally, we can estimate the value of $E[g(\\mu,\\sigma^2)\\vert X]$SummaryComparing the MC method and the MCMC, the purpose of both algorithms is to obtain a sequence of parameter values ${ \\theta^{(1)}, \\theta^{(2)}, \\cdots, \\theta^{(M)} }$ such thatfor any functions $f$ of interest.However, there exist some differences in their methodology. The MC simulation generates independent samples from a target distribution, whereas the MCMC simulation generates dependent samples from the full conditional distributions. So, we need to check auto-correlation over samples. And, the Gibbs sampling plays important roles in the MCMC method.References  Walsh - 2004 - Markov chain monte carlo and gibbs sampling  Andrieu et al. - 2003 - An introduction to MCMC for machine learningSee Also  Markov Chain Monte Carlo Technology",
			"date": 1377356400000,
			"tags": ["statistics", "bayesian"],
			"title": "Markov chain Monte Carlo(MCMC) and Gibbs Sampling"
		},
		"HcOsCD7GCYaTgLgwGv1cNc": {
			"category": ["data_modeling"],
			"content": "IntroductionSuppose that we are under the situation that we want to discover the topics given a set of documents. What is the discovery of topics? It is to find out the values of the topic-related parameters in a topic model, such as pLSA or LDA. But, we cannot find the exact values of parameters of the model due to its complicatedness in calculating. So, we will estimate the parameters and this process is called inference in Bayesian approach. In this post, we will take the Gibbs sampling strategy for inference in LDA model.Using Gibbs Sampling to Discover TopicsGenerally, the estimation problem in LDA becomes one of maximizing the likelihood $p(\\textbf{w}\\vert \\phi,\\alpha)=\\intp(\\textbf{w}\\vert \\phi,\\theta)p(\\theta\\vert \\alpha)d\\theta$, where $\\textbf{w}$ represents the given documents and $p(\\theta;\\alpha)$ is a Dirichlet distribution with the parameter $\\alpha$. But, the integral in this expression is known as being intractable in [1].Instead of the above inference strategy, we will consider the posterior distribution over the topic assignments $p(z \\vert \\textbf{w})$ because what we are interested in is the distribution of topics given a document. Then, we will obtain estimates of $\\theta$ and $\\phi$ by examining this posterior distribution. And, we will take collapsed Gibbs sampling for inference problem. As for the collapsed Gibbs sampling, the general Gibbs sampling step for $z_i$, $p(z_i \\vert \\textbf{z}_ {-i}, \\textbf{w}, \\theta, \\phi, \\alpha, \\beta)$, is replaced with a sample taken from the marginal distribution $p(z_i \\vert \\textbf{z}_ {-i}, \\textbf{w}, \\alpha, \\beta)$, with the variables $\\theta$ and $\\phi$ integrated out, because this variation is tractable when $\\alpha$ and $\\beta$ are conjugate to $\\theta$ and $\\phi$ respectively.The total probability of the model iswhere $c_d$ denotes the length of document $d$.Then, the joint distribution $p(\\textbf{w},\\textbf{z};\\alpha, \\beta)=p(\\textbf{w}\\vert \\textbf{z},\\beta)p(\\textbf{z}\\vert \\alpha)$ by integrating out $\\phi$ and $\\theta$ iswhere $c_k^{(\\cdot,v)}$ is the number of times that $v$th vocabulary has been assigned to topic $k$ in all documents, and $c_k^{(d,\\cdot)}$ is the number of times that a word in document $d$ has been assigned to topic $k$.The goal of Gibbs sampling here is to approximate the distribution:Since $p(\\textbf{w}\\vert \\alpha,\\beta)$ is invariable for any of $z$, the full-conditional distributions can be derived from the joint distribution $p(\\textbf{z},\\textbf{w}\\vert \\alpha,\\beta)$ directly.where $z_{(m,n)}$ denotes the topic assignment of the $n$th word in document $m$, which corresponds to $r$th vocabulary, and $m \\in { 1, \\cdots, M }$ and $n \\in { 1, \\cdots, N_m }$.Let $c_{k, -(m,n)}^{(d,t)}$ be the number of times that $t$th word in document $d$ has been assigned to topic $k$ excluding a topic which is assigned to the $n$th word in document $m$. The topic assigned to the $n$th word in document $m$ is $l$.So, $\\Gamma \\left ( c_l^{(m,\\cdot)} + \\alpha_l \\right )$, for example, can be rewritten as $\\Gamma \\left ( c_{l,-(m,n)}^{(m,\\cdot)} + \\alpha_l + 1 \\right )$.Then, the above equation is further simplified by treating terms not dependent on $l$ as constants, and we can conclude as follows using the property of Gamma function, $\\Gamma(\\alpha + 1)=\\alpha \\Gamma(\\alpha)$:Having obtained the full conditional distribution, the MCMC algorithm is then straightforward. The $z_{(m,n)}$ variables are initialized to values in ${1,2,\\cdots,K}$, determining the initial state of the Markov chain. Then, the chain is run for a number of iterations, each time finding a new state by sampling each $z_i$ from the full conditional distribution. After enough iterations for the chain to approach the target distribution, the current values of the $z_i$ are recorded.With a set of samples from the posterior distribution $p(\\textbf{z}\\vert \\textbf{w})$, statistics can be computed by integrating across the full set of samples. For any single sample we can estimate $\\phi$ and $\\theta$ from the value $\\textbf{z}$ by      The probability of topic $l$ in the document $m$          The probability of $n$th word $(=r)$ under topic $l$    Model SelectionThe LDA model is conditioned on three parameters: the Dirichlet hyperparameters $\\alpha$ and $\\beta$ and the number of topics $K$. The above sampling algorithm is easily extended to allow $\\alpha$, $\\beta$, and $\\textbf{z}$ to be sampled, but this extension can slow the convergence of the Markov chain. One strategy is to fix $\\alpha$ and $\\beta$ and explore the consequences of varying $K$. The choice of $\\alpha$ and $\\beta$ can have important implications for the results produced by the model. There are many ways for learning $\\alpha$ and $\\beta$, among which Minka’s fixed-point iteration is widely used. [3]We update $\\alpha$ and $\\beta$ as follows:In standard LDA model, a single scalar parameter $\\beta$ is used as a hyperparameter on the exchangeable Dirichlet.Collapsed Gibbs samplingA collapsed Gibbs sampler integrates out one or more variables when sampling for some other variable. For example, suppose that a model consists of three variables $A$, $B$, and $C$. A simple Gibbs sampler would sample from $p(A\\vert B,C)$, then $p(B\\vert A,C)$, then $p(C\\vert A,B)$. A collapsed Gibbs sampler might replace the sampling step for $A$ with a sample taken from the marginal distribution $p(A\\vert C)$, with variable $B$ integrated out in this case. Alternatively, variable $B$ could be collapsed out entirely, alternately sampling from $p(A\\vert C)$ and $p(C\\vert A)$ and not sampling over $B$.The distribution over a variable $A$ that arises when collapsing a parent variable $B$ is called a compound distribution; sampling from this distribution is generally tractable when $B$ is the conjugate prior for $A$, particularly when $A$ and $B$ are members of the exponential family.It is quite common to collapse out the Dirichlet distributions that are typically used as prior distributions over the categorical variables. The result of this collapsing introduces dependencies among all the categorical variables dependent on a given Dirichlet prior, and the joint distribution of these variables after collapsing is a Dirichlet-multinomial distribution. The conditional distribution of a given categorical variable in this distribution, conditioned on the others, assumes an extremely simple form that makes Gibbs sampling even easier than if the collapsing had not been done. The rules are as follows:  Collapsing out a Dirichlet prior node affects only the parent and children nodes of the prior. Since the parent is often a constant, it is typically only the children that we need to worry about.  Collapsing out a Dirichlet prior introduces dependencies among all the categorical children dependent on that prior — but no extra dependencies among any other categorical children. (This is important to keep in mind, for example, when there are multiple Dirichlet priors related by the same hyperprior. Each Dirichlet prior can be independently collapsed and affects only its direct children.)  After collapsing, the conditional distribution of one dependent children on the others assumes a very simple form: The probability of seeing a given value is proportional to the sum of the corresponding hyperprior for this value, and the count of all of the other dependent nodes assuming the same value. Nodes not dependent on the same prior must not be counted. Note that the same rule applies in other iterative inference methods, such as variational Bayes or expectation maximization; however, if the method involves keeping partial counts, then the partial counts for the value in question must be summed across all the other dependent nodes. Sometimes this summed up partial count is termed the expected count or similar. Note also that the probability is proportional to the resulting value; the actual probability must be determined by normalizing across all the possible values that the categorical variable can take (i.e. adding up the computed result for each possible value of the categorical variable, and dividing all the computed results by this sum)  If a given categorical node has dependent children (e.g. when it is a latent variable in a mixture model), the value computed in the previous step (expected count plus prior, or whatever is computed) must be multiplied by the actual conditional probabilities (not a computed value that is proportional to the probability!) of all children given their parents. See the article on the Dirichlet-multinomial distribution for a detailed discussion.  In the case where the group membership of the nodes dependent on a given Dirichlet prior may change dynamically depending on some other variable (e.g. a categorical variable indexed by another latent categorical variable, as in a topic model), the same expected counts are still computed, but need to be done carefully so that the correct set of variables is included. See the article on the Dirichlet-multinomial distribution for more discussion, including in the context of a topic modelReferences  Blei, Ng, Jordan - 2003 - Latent dirichlet allocation  Griffiths, Steyvers - 2004 - Finding scientific topics  Minka - 2000 - Estimating a Dirichlet distributionSee Also  http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation  http://en.wikipedia.org/wiki/Dirichlet-multinomial_distribution#A_combined_example:_LDA_topic_models",
			"date": 1401894000000,
			"tags": ["statistics", "bayesian", "topic_model"],
			"title": "Gibbs Sampling for Topic Models"
		},
		"HrYfF3KFlzuBc6TdRQVD6Q": {
			"category": ["fundamentals", "statistics"],
			"content": "Introduction고전통계학에서는 단순히 주어진 data information (likelihood)을 최대화하는 parameter $\\hat{\\theta}_{\\text{MLE}}$를 취한다. 하지만 베이지안 통계학에서는 데이터와 prior information을 종합하여 관심의 대상이 되는 parameter $\\theta$의 불확실성을 확률 분포(posterior)로 나타내는 것을 목표로 한다.베이지안 통계학은 다음과 같은 기본적인 구조를 갖는다.      Determine a prior distribution of $\\theta$, $p(\\theta)$    Compute a posterior distribution $p(\\theta|X)$ using data $X$ and prior distribution    Inference using the posterior distribution  여기서 파라미터의 prior distribution을 구하는 방법에 있어 여러 가지 문제점이 있는데, 그 중 하나는 분석자의 특성에 따라 prior distribution이 달라서 상당히 주관적이라는 것이다. 그래서 객관적인 prior distribution이 제안되었는데 그 중 하나가 noninformative prior distribution으로 Laplace에 의해 제안된 uniform distribution을 이용하는 것이다. 아무런 정보가 없는 상태에서는 모든 파라미터 값은 동일하게 취급되어야 한다는 것인데, 이는 이론적으로 문제가 있음이 밝혀졌고 (lack of invariance for transformation), 그 대안 중의 하나가 Jeffrey의 prior distribution이다.  Jeffrey’s prior information    Expectation of Fisher information, $I(\\theta)$  일단 prior distribution이 주어지면, posterior distribution은 Bayes’s rule에 의해 쉽게 구할 수 있으나 실제 데이터를 이용한 계산은 보통 어려운 작업이 아니다. 하지만 posterior distribution이 prior distribution과 동일한 분포족(distribution family)이면 posterior distribution의 계산이 쉬워지게 되는 경우가 있는데, 이러한 prior distribution을 conjugate prior라고 한다.그리고 베이지안 통계학에서 파라미터에 대한 추론(inference)은 전적으로 posterior distribution에 의존하여 이루어진다.Bayesian Inference베이지안 통계학에서 데이터가 주어졌을 때 posterior distribution을 통해 어느 모델이 가장 좋은 데이터 생성 모델인지 추론(inference)할 수 있다. 즉, 가장 좋은 데이터 모델의 parameter $\\theta$를 추정할 수 있다. 미지의 파라미터 $\\theta$에 대한 정보를 얻기 위해 특정 확률 변수 $X$의 값을 같은 조건 하에서 반복적으로 관측하여 표본을 추출한다. (identically independently distributed, iid) 이렇게 추출된 각각의 표본들을 모아놓은 집합을 sample space $\\Omega$라고 하고, Euclidean space $\\mathbb{R}^n$의 부분집합이 된다.확률 변수 $X$의 distribution은 당연히 미지의 parameter $\\theta$가 주어져야 정의된다. 따라서 $X$의 probability distribution은 $X$의 density function $f(x|\\theta)$를 이용해 나타낼 수 있다. 표본들 ${ x_1, x_2, \\dots, x_n } (x_i \\in \\Omega)$가 주어졌을 때, likelihood function $L(x|\\theta)$는 다음과 같다.고전통계학에서는 이 likelihood function을 이용해 parameter $\\theta$에 대해 추론하고 의사 결정 과정에 이용하게 된다. 예를 들어, 이 likelihood function을 최대로 하는 parameter $\\theta$를 maximum likelihood estimate (MLE)라고 부르고 $\\theta$의 추정에 사용하는 것이다.그러나 여기에 추가적으로 표본들로부터 얻은 parameter $\\theta$에 대해 활용 가능한 prior information이 있다면 더 나은 의사 결정이 가능할 것이다. 이 prior information을 parameter space $\\Theta$ 위에서 정의된 probability distribution으로 나타낸 것을 prior distribution이라고 한다. 베이지안 통계학에서는 parameter에 대한 prior information과 확률 실험에 의해 얻은 데이터 정보(likelihood)를 종합한 posterior distribution을 이용해 parameter $\\theta$에 대한 추론, 더 나아가 의사 결정에 이용한다.(베이지안 통계학에서는 위와 같이 posterior mode를 취하기도 하지만 posterior mean 또는 posterior median도 parameter 추정에 사용되곤 한다. Posterior는 parameter $\\theta$의 distribution으로 나타나기 때문에 취할 수 있는 값은 여러가지이다.)  Reference: Statistical Inference      Parameter estimation              Point estimation        Confidence interval estimation              Hypothesis Testing  Iterative learning (Data Update)전통적인 모델링에서는 training set에 가장 잘 맞는 모델을 세우고, unseen data에도 적용될 수 있기를 기대한다. 즉, 새로 주어지는 데이터들에 대한 learning 과정은 수행되지 않는다.하지만, iterative learning에서는 새로운 observation(데이터)이 발생할 때마다 기존의 prior $p^{(i)}(\\theta)$와 새로이 구한 likelihood를 곱하고 normalize하여 새로운 posterior $p^{(i+1)}(\\theta)$를 구해내고 기존의 prior를 대체하여 prior를 계속 새로 갱신시켜 나가는 것이다. (즉, 새로이 관측된 데이터 정보를 포함하는 새로운 prior를 취하는 것이다.)이러한 iterative learning을 위해서는 초기상태의 prior $p^{(0)}(\\theta)$가 필요한데 이를 initial belief라고 한다.      Given data: $X_1={x_{11}, x_{12}, \\cdots, x_{1n}} \\sim f(X_1|\\theta)$    Initial belief: $\\theta \\sim p_0(\\theta)$    New data: $X_2={x_{21}, x_{22}, \\cdots, x_{2m}}$    Updated prior: $\\textcolor{Red}{p(\\theta|X_1)} = \\frac{f(X_1|\\theta)p_0(\\theta)}{p(X_1)}$    Likelihood of new data $X_2$: $\\textcolor{Blue}{f(X_2|\\theta,X_1)}$    New posterior:  Conjugate prior관측된 데이터의 종류 및 특성에 따라 어떤 분포든 initial belief로 사용될 수 있다. 하지만 반복적인 계산 과정을 통하면 posterior distribution은 수학적으로 매우 복잡한 분포로 빠져들 수 있다. 만약에 prior와 posterior가 같은 distribution family이라면 posterior를 구하는 계산이 훨씬 쉬워진다. (또한 posterior predictive distribution에 대한 계산도 역시 쉬워진다.) 이렇게 posterior와 같은 종류의 확률 분포를 갖는 prior distribution을 conjugate prior라고 한다.            Likelihood $p(X|\\theta)$      Conjugate prior $p_0(\\theta)$      Posterior $p(\\theta|X)$                  $Normal(\\mu, \\sigma)$      $Normal(\\mu_0, \\sigma_0)$      $Normal(\\mu_1, \\sigma_1)$              $Binomial(n,p)$      $Beta(\\alpha, \\beta)$      $Beta(\\alpha+n, \\beta+N-n)$              $Poisson(\\lambda)$      $Gamma(\\alpha, \\beta)$      $Gamma(\\alpha+n, \\beta+1)$              $Multinomial(p_1,\\dots,p_k)$      $Dirichlet(\\alpha_1,\\dots,\\alpha_k)$      $Dirichlet(\\alpha_1+n_1, \\dots, \\alpha_k+n_k)$      하지만, conjugate prior를 쓰면 fixed된 density structure라는 이유로 flexible하지 않다는 문제가 있다. 이에 대한 해결책은 mixture of densities의 개념을 사용하는 것이다. (자세한 내용은 생략)Posterior predictive distribution기존에 주어진 데이터로 새로운 데이터의 값을 예측하는 posterior distribution이다. 주어진 데이터 $X$로 모델이 생성되었다면 posterior distribution $p(\\theta|X)$를 가지고 새로운 데이터 $Y$에 대한 prediction이 다음과 같이 가능하다.위와 같이, 새로 관측가능한 데이터 $Y$에 대한 distribution을 posterior predictive distribution이라고 한다.Advantages of Bayesian statistics  결과에 대한 해석이 전통적인 통계학보다 훨씬 쉽고 우리의 직관과 잘 부합된다.통계학의 목표는 미지의 parameter에 대해 추론하고 그 추론의 불확실성을 계량화하는 것이다. 그런데 불확실성을 나타내는 가장 좋은 방법이 probability이며, 베이지안 통계학은 probability를 직접 이용하는 통계적 기법이다.  실제로 많은 문제들에서 parameter의 prior information을 쉽게 구할 수 있다.전통적인 통계학에서는 공통분산을 사용하는데 이는 prior information을 굉장히 단순하게 사용한 것이다. 베이지안 통계학은 더욱 효과적으로 prior information을 이용한 추론 결과를 제시할 수 있다.  베이지안 기법이 전통적인 통계학의 관점에서도 좋은 추론 방법을 제공한다.전통적인 통계학에서 사용되는 많은 통계량들은 베이지안 추정량 또는 베이지안 추정량의 극한으로 구할 수 있다.복잡한 문제에서 parameter의 추론이 전통적인 통계학보다 쉽다.  전통적인 통계학에서 많이 쓰이는 MLE(Maximum Likelihood Estimator)는 likelihood function이 복잡하거나 많은 local optima를 가지는 경우가 많아 estimation이 어려울 뿐 아니라 일반적으로 estimation 결과가 나쁠 때가 많다. 반면, 베이지안 통계학에서 개발된 MCMC기법을 사용하면 parameter의 prior distribution을 쉽게 구할 수 있을 뿐 아니라, posterior distribution을 통한 parameter 추론을 더욱 정확하게 할 수 있다.",
			"date": 1376578800000,
			"tags": ["statistics", "bayesian"],
			"title": "Bayesian Statistics"
		}
	}
}
